# digit-classifier-neural-network

This is part of the <a href='https://www.kaggle.com/competitions/digit-recognizer'>Kaggle Digit Recognizer Competition</a>, which employs the MNIST database. The MNIST database is a set of images of handwritten digits. Participants are introduced to various models ranging from regression to neural networks to learn to classify these images. 

In this project, I have decided to build a dense neural network from scratch (only using numpy) in order to better understand the functionalities and mathematics behind each step in the neural network. I have also experimented with different hyper-parameters, activation and initialization functions and number of layers and nodes. Feel free to look through my notebook for more information. 

## Initialization functions
- Xavier Initialization

## Activation functions
- Rectified Linear Unit (ReLU)
- Exponential Linear Unit (ELU)
- Sigmoid
- Softmax

Accuracy of latest submission (#4) : 0.9046

In the future, I hope to expand on this project to explore other types of neural network such as Convolutional Neural Network. Similarly, I aim to build a CNN model from scratch for digit classification. So, stay tuned for more! :)

# References
- https://www.youtube.com/watch?v=w8yWXqWQYmU&t=229s&pp=ygUjbmV1cmFsIG5ldHdvcmsgZGlnaXQgY2xhc3NpZmljYXRpb24%3D
- https://youtu.be/0idoEomDc9E
- https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/
